plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.25))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
}
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(depth=model_best)
tm_train=NA
if(run.train){
tm_train_gbm <- system.time(fit_train_gbm <- train(feat_train, label_train, par_best))
#tm_train <- system.time(fit_train <- train(feat_train, label_train))
save(fit_train_gbm, file="../output/fit_train_gbm.RData")
}
if(run.train){
tm_train_xgboost <- system.time(fit_train_xgboost <- xgb_train(feat_train, label_train))
save(fit_train_xgboost, file="../output/fit_train_xgboost.RData")
}
source("../lib/superResolution.R")
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")
###GBM test
if(run.test){
load(file="../output/fit_train_gbm.RData")
tm_test_gbm <- system.time(PSNR_gbm <- superResolution_gbm(test_LR_dir, test_HR_dir, fit_train_gbm))
}
#XGBoost test
if(run.test){
load(file="../output/fit_train_gbm.RData")
tm_test_xgboost <- system.time(PSNR_xgboost <- superResolution_xgboost(test_LR_dir, test_HR_dir, fit_train_xgboost))
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for training gbm model=", tm_train_gbm[1], "s \n")
cat("Time for gbm super-resolution=", tm_test_gbm[1], "s \n")
cat("Time for training xgboost model=", tm_train_xgboost[1], "s \n")
cat("Time for xgboost super-resolution=", tm_test_xgboost[1], "s \n")
print(paste("PSNR_gbm:", PSNR_gbm, sep=" "))
print(paste("PSNR_xgboost:", PSNR_xgboost, sep=" "))
rm(list = ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
set.seed(2018)
setwd("../")
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
run.cv=FALSE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
run.train=TRUE
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
source("../lib/feature2.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
feat_train <- dat_train$feature
label_train <- dat_train$label
save(dat_train, file="../output/feature_train.RData")
}
#########################################################
### Train a classification model with training features ###
#########################################################
### Author: Chengliang Tang
### Project 3
######GBM########
train <- function(dat_train, label_train, par=NULL){
### Train a Gradient Boosting Model (GBM) using processed features from training images
### Input:
###  -  features from LR images
###  -  responses from HR images
### Output: a list for trained models
### load libraries
library("gbm")
### creat model list
modelList <- list()
### Train with gradient boosting model
if(is.null(par)){
depth <- 1
} else {
depth <- par$depth
}
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
fit_gbm <- gbm.fit(x=featMat, y=labMat,
n.trees = 100, # 200
distribution="gaussian",
minNode = 14,
interaction.depth=depth,
bag.fraction = 0.5,
shrinkage = .09,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
modelList[[i]] <- list(fit=fit_gbm, iter=best_iter)
}
return(modelList)
}
########XGB########
xgb_train <- function(dat_train, label_train, tune = FALSE) {
library(xgboost)
# if (tune) {
#   params <- xgb_para(dat_train = dat_train, label_train = label_train, K = 5, nround = 200)
#
#   param.round <- xgb.set.M(dat_train = dat_train, label_train = label_train, M.range = c(80, 200),
#                            max_depth = params[[2]]$max_depth, eta = params[[2]]$eta,
#                            step = 10, K = 5)
#
#   best_para<-list(max_depth = params[[2]]$max_depth, eta = params[[2]]$eta, nrounds = param.round[[1]],
#                   gamma = 0, nthread = 2, subsample = 0.5,
#                   objective = "multi:softprob", num_class = 3)
# } else {
#   best_para<-list(booster = 'gblinear',
#                   objective = "reg:linear", eval_metric = 'RMSE', nrounds = 100)
#
#
# }
modelList <- list()
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-c1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
# xgbFit = xgboost(data = as.matrix(featMat), nfold = 5, label = as.matrix(labMat),
#                  nrounds = 100, verbose = FALSE, objective = "reg:linear", eval_metric = "rmse",
#                  nthread = 8, eta = 0.01, gamma = 0.0468, max_depth = 7, lambda = 1, alpha = 0,min_child_weight = 1.7817,
#                  subsample = 0.5213, colsample_bytree = 0.4603)
############
xgbFit = xgboost(data = data.matrix(featMat),
label = as.numeric(labMat) ,
booster = "gblinear",
nrounds = 150,
max_depth = 7,
lambda = 0.5,
eta = .15,
gamma = .1,
colsample_bytree = 0.4603,
early_stopping_rounds = 30,
min_child_weight = 6.5,
subsample = .6,
eval_metric = "rmse",
alpha = 0.5,
objective = "reg:linear")
modelList[[i]] <- list(fit=xgbFit)
}
return(modelList)
}
tm_train=NA
if(run.train){
tm_train_gbm <- system.time(fit_train_gbm <- train(feat_train, label_train, par_best))
#tm_train <- system.time(fit_train <- train(feat_train, label_train))
save(fit_train_gbm, file="../output/fit_train_gbm.RData")
}
tm_train=NA
if(run.train){
#tm_train_gbm <- system.time(fit_train_gbm <- train(feat_train, label_train, par_best))
tm_train <- system.time(fit_train <- train(feat_train, label_train))
save(fit_train_gbm, file="../output/fit_train_gbm.RData")
}
#########################################################
### Train a classification model with training features ###
#########################################################
### Author: Chengliang Tang
### Project 3
######GBM########
train <- function(dat_train, label_train, par=NULL){
### Train a Gradient Boosting Model (GBM) using processed features from training images
### Input:
###  -  features from LR images
###  -  responses from HR images
### Output: a list for trained models
### load libraries
library("gbm")
### creat model list
modelList <- list()
### Train with gradient boosting model
if(is.null(par)){
depth <- 1
} else {
depth <- par$depth
}
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
fit_gbm <- gbm.fit(x=featMat, y=labMat,
n.trees = 100, # 200
distribution="gaussian",
# minNode = 14,
interaction.depth=depth,
bag.fraction = 0.5,
shrinkage = .09,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
modelList[[i]] <- list(fit=fit_gbm, iter=best_iter)
}
return(modelList)
}
########XGB########
xgb_train <- function(dat_train, label_train, tune = FALSE) {
library(xgboost)
# if (tune) {
#   params <- xgb_para(dat_train = dat_train, label_train = label_train, K = 5, nround = 200)
#
#   param.round <- xgb.set.M(dat_train = dat_train, label_train = label_train, M.range = c(80, 200),
#                            max_depth = params[[2]]$max_depth, eta = params[[2]]$eta,
#                            step = 10, K = 5)
#
#   best_para<-list(max_depth = params[[2]]$max_depth, eta = params[[2]]$eta, nrounds = param.round[[1]],
#                   gamma = 0, nthread = 2, subsample = 0.5,
#                   objective = "multi:softprob", num_class = 3)
# } else {
#   best_para<-list(booster = 'gblinear',
#                   objective = "reg:linear", eval_metric = 'RMSE', nrounds = 100)
#
#
# }
modelList <- list()
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-c1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
# xgbFit = xgboost(data = as.matrix(featMat), nfold = 5, label = as.matrix(labMat),
#                  nrounds = 100, verbose = FALSE, objective = "reg:linear", eval_metric = "rmse",
#                  nthread = 8, eta = 0.01, gamma = 0.0468, max_depth = 7, lambda = 1, alpha = 0,min_child_weight = 1.7817,
#                  subsample = 0.5213, colsample_bytree = 0.4603)
############
xgbFit = xgboost(data = data.matrix(featMat),
label = as.numeric(labMat) ,
booster = "gblinear",
nrounds = 150,
max_depth = 7,
lambda = 0.5,
eta = .15,
gamma = .1,
colsample_bytree = 0.4603,
early_stopping_rounds = 30,
min_child_weight = 6.5,
subsample = .6,
eval_metric = "rmse",
alpha = 0.5,
objective = "reg:linear")
modelList[[i]] <- list(fit=xgbFit)
}
return(modelList)
}
tm_train=NA
if(run.train){
#tm_train_gbm <- system.time(fit_train_gbm <- train(feat_train, label_train, par_best))
tm_train <- system.time(fit_train <- train(feat_train, label_train))
save(fit_train_gbm, file="../output/fit_train_gbm.RData")
}
source("../lib/superResolution.R")
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")
###GBM test
if(run.test){
load(file="../output/fit_train_gbm.RData")
tm_test_gbm <- system.time(PSNR_gbm <- superResolution_gbm(test_LR_dir, test_HR_dir, fit_train_gbm))
}
source("../lib/train.R")
source("../lib/test.R")
###GBM test
if(run.test){
load(file="../output/fit_train_gbm.RData")
tm_test_gbm <- system.time(PSNR_gbm <- superResolution_gbm(test_LR_dir, test_HR_dir, fit_train_gbm))
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for training gbm model=", tm_train_gbm[1], "s \n")
print(paste("PSNR_gbm:", PSNR_gbm, sep=" "))
PSNR_gbm
source("../lib/superResolution.R")
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")
###GBM test
if(run.test){
load(file="../output/fit_train_gbm.RData")
tm_test_gbm <- system.time(PSNR_gbm <- superResolution_gbm(test_LR_dir, test_HR_dir, fit_train_gbm))
}
PSNR_xgboost
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
if(!require("caret")){
install.packages("caret")
}
library("EBImage")
library("gbm")
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 3  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
# df -  is the feature combinations for baseline gbm
df <- data.frame(matrix(ncol = 3, nrow = 36))
x <- c("numTrees", "minNode", "shrinkage")
colnames(df) <- x
numTrees = seq(60, 110, 20)
minNode = seq(5, 15, 3)
shrinkage = seq(0.05, 0.1, 0.02)
df$numTrees <- rep(numTrees, each = length(minNode)*length(shrinkage))
df$minNode <- rep(rep(minNode, each = length(shrinkage)), length(numTrees))
df$shrinkage <- rep(shrinkage, length(numTrees)*length(minNode))
model_values <- df
#model_labels = paste("GBM with numTrees =", model_values)
extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
feat_train <- dat_train$feature
label_train <- dat_train$label
}
#save(dat_train, file="../output/feature_train.RData")
source("../lib/train_baseline.R")
source("../lib/test_baseline.R")
source("../lib/cross_validation_baseline.R")
if(run.cv){
err_cv <- array(dim=c(nrow(model_values), 2))
for(k in 1:nrow(model_values)){
print(paste('Currently evaluating parameter combination ', k))
cat("k=", k, "\n")
err_cv[k,] <- cv.function(feat_train, label_train,  K, depth = 1,
numTrees = model_values$numTrees[k] ,
minNode = model_values$minNode[k] ,
shrinkage = model_values$shrinkage[k])
}
save(err_cv, file="../output/err_cv_baseline.RData")
}
#########################################################
### Train a classification model with training features ###
#########################################################
### Author: Chengliang Tang
### Project 3
######GBM########
train <- function(dat_train, label_train, par=NULL){
library("gbm")
### creat model list
modelList <- list()
### Train with gradient boosting model
if(is.null(par)){
depth <- 1
} else {
depth <- par$depth
}
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
fit_gbm <- gbm.fit(x=featMat, y=labMat,
n.trees = 100,
distribution="gaussian",
n.minobsinnode = 14,
interaction.depth=depth,
keep.data = TRUE,
bag.fraction = 0.5,
shrinkage = .09,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
modelList[[i]] <- list(fit=fit_gbm, iter=best_iter)
}
return(modelList)
}
########XGB########
xgb_train <- function(dat_train, label_train, tune = FALSE) {
library(xgboost)
modelList <- list()
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-c1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
############
xgbFit = xgboost(data = data.matrix(featMat),
label = as.numeric(labMat) ,
booster = "gblinear",
nrounds = 150,
max_depth = 7,
lambda = 0.5,
eta = .15,
gamma = .1,
colsample_bytree = 0.4603,
early_stopping_rounds = 30,
min_child_weight = 6.5,
subsample = .6,
eval_metric = "rmse",
alpha = 0.5,
objective = "reg:linear")
modelList[[i]] <- list(fit=xgbFit)
}
return(modelList)
}
#########################################################
### Train a classification model with training features ###
#########################################################
### Author: Chengliang Tang
### Project 3
######GBM########
train <- function(dat_train, label_train, par=NULL){
library("gbm")
### creat model list
modelList <- list()
### Train with gradient boosting model
if(is.null(par)){
depth <- 1
} else {
depth <- par$depth
}
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
fit_gbm <- gbm.fit(x=featMat, y=labMat,
n.trees = 100,
distribution="gaussian",
n.minobsinnode = 14,
interaction.depth=depth,
keep.data = TRUE,
bag.fraction = 0.5,
shrinkage = .09,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
modelList[[i]] <- list(fit=fit_gbm, iter=best_iter)
}
return(modelList)
}
########XGB########
xgb_train <- function(dat_train, label_train, tune = FALSE) {
library(xgboost)
modelList <- list()
for (i in 1:12){
## calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-c1) %/% 4 + 1
featMat <- dat_train[, , c2]
labMat <- label_train[, c1, c2]
############
# xgbFit = xgboost(data = data.matrix(featMat),
#         label = as.numeric(labMat) ,
#         booster = "gblinear",
#         nrounds = 150,
#         max_depth = 7,
#         lambda = 0.5,
#         eta = .15,
#         gamma = .1,
#         colsample_bytree = 0.4603,
#         early_stopping_rounds = 30,
#         min_child_weight = 6.5,
#         subsample = .6,
#         eval_metric = "rmse",
#         alpha = 0.5,
#         objective = "reg:linear")
#
paras<-list(objective="reg:linear",
eta=0.5,
nthread = 2,
max.depth=8)
xgbFit <- xgboost(data = data.matrix(featMat), label = as.numeric(labMat),
nround = 14,
params=paras)
modelList[[i]] <- list(fit=xgbFit)
}
return(modelList)
}
rm(list = ls())
1
